---
name: AI model
author: Arne Tarara <arne@green-coding.io>
description: Run an inference with a small AI model on the GPU

compose-file: !include compose.yml

services:
  gcb-ai-model:
    docker-run-args:
      - --gpus=all
      - -v ollama:/root/.ollama

flow:
  # Models will already be present!
  # - name: Download gemma3-1b
  #   container: gcb-ai-model
  #   commands:
  #     - type: console
  #       command: ollama pull gemma3:1b
  #       read-notes-stdout: true
  #       log-stdout: true

  - name: Load gemma3-1b into memory
    container: gcb-ai-model
    commands:
      - type: console
        command: ollama run gemma3:1b ""
        read-notes-stdout: true
        log-stdout: true

  - name: Run Inference on gemma3-1b
    container: gcb-ai-model
    commands:
      - type: console
        command: ollama run gemma3:1b "Tell me a long joke?"
        read-notes-stdout: true
        log-stdout: true

  - name: Load llama2 into memory
    container: gcb-ai-model
    commands:
      - type: console
        command: ollama run llama2 ""
        log-stdout: true

  - name: Run Inference on llama2
    container: gcb-ai-model
    commands:
      - type: console
        command: ollama run llama2 "Tell me a long joke?"
        log-stdout: true